# -*- coding: utf-8 -*-
"""Autonomous_Project_Phase1_PartC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NDy4uGGXt-rv7szFie0mxbfgIZE02tP1
"""

import numpy as np
import matplotlib.pyplot as plt

"""## **R1**"""

# Defining the grid size
GRID_SIZE = 5
# Defining the states for the grid
states = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]
# Defining the actions: Up, Down, Left, Right
actions = ['Up', 'Down', 'Left', 'Right']

"""## Defining the transition probabilities and rewards for each action"""

transition_probs = {
    'Up': {'Up': 0.8, 'Left': 0.1, 'Right': 0.1, 'Down': 0.0},
    'Down': {'Down': 0.8, 'Left': 0.1, 'Right': 0.1, 'Up': 0.0},
    'Left': {'Left': 0.8, 'Up': 0.1, 'Down': 0.1, 'Right': 0.0},
    'Right': {'Right': 0.8, 'Up': 0.1, 'Down': 0.1, 'Left': 0.0}
}

rewards = {
    (0, 1): 20,  # Regular customer at position A
    (2, 2): -0.5,  # Live-in reward at starting position
    (4, 4): 10,  # Example exit state with a high reward
    (3, 1): -10  # Restricted area
}
gamma = 0.9  # Discount factor
# Initialize state values
V = {state: 0 for state in states}

"""## Value Iteration algorithm"""

while True:
    delta = 0
    for state in states:
        if state in rewards:
            continue

        max_value = float('-inf')
        for action in actions:
            new_value = 0
            for next_action, prob in transition_probs[action].items():
                next_state = state
                if next_action == 'Up':
                    next_state = (max(state[0] - 1, 0), state[1])
                elif next_action == 'Down':
                    next_state = (min(state[0] + 1, GRID_SIZE - 1), state[1])
                elif next_action == 'Left':
                    next_state = (state[0], max(state[1] - 1, 0))
                elif next_action == 'Right':
                    next_state = (state[0], min(state[1] + 1, GRID_SIZE - 1))

                new_value += prob * (rewards.get(next_state, -0.5) + gamma * V[next_state])

            if new_value > max_value:
                max_value = new_value

        delta = max(delta, abs(max_value - V[state]))
        V[state] = max_value

    if delta < 1e-6:
        break

# Extract the optimal policy
optimal_policy = {}
for state in states:
    if state in rewards:
        optimal_policy[state] = 'Exit'
    else:
        best_action = None
        best_value = float('-inf')
        for action in actions:
            action_value = 0
            for next_action, prob in transition_probs[action].items():
                next_state = state
                if next_action == 'Up':
                    next_state = (max(state[0] - 1, 0), state[1])
                elif next_action == 'Down':
                    next_state = (min(state[0] + 1, GRID_SIZE - 1), state[1])
                elif next_action == 'Left':
                    next_state = (state[0], max(state[1] - 1, 0))
                elif next_action == 'Right':
                    next_state = (state[0], min(state[1] + 1, GRID_SIZE - 1))

                action_value += transition_probs[action][next_action] * (rewards.get(next_state, -0.5) + gamma * V[next_state])

            if action_value > best_value:
                best_value = action_value
                best_action = action

        optimal_policy[state] = best_action

"""## Visual representation of the optimal policy as numeric values in each cell"""

for i in range(GRID_SIZE):
    for j in range(GRID_SIZE):
        if (i, j) in optimal_policy:
            print(optimal_policy[(i, j)], end='\t')
    print()

"""## **R2**

## Visualize the optimal policy using your preferred method
"""

# Define the grid size
GRID_SIZE = 5

# Define the states for the grid
states = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]

# Define the actions: Up, Down, Left, Right
actions = ['Up', 'Down', 'Left', 'Right']

# Define the transition probabilities and rewards for each action
transition_probs = {
    'Up': {'Up': 0.8, 'Left': 0.1, 'Right': 0.1, 'Down': 0.0},
    'Down': {'Down': 0.8, 'Left': 0.1, 'Right': 0.1, 'Up': 0.0},
    'Left': {'Left': 0.8, 'Up': 0.1, 'Down': 0.1, 'Right': 0.0},
    'Right': {'Right': 0.8, 'Up': 0.1, 'Down': 0.1, 'Left': 0.0}
}

rewards = {
    (1, 0): 30,  # Premium customer at position B
    (2, 2): -0.5,  # Live-in reward at starting position
    (4, 4): 10,  # Example exit state with a high reward
    (3, 1): -10  # Restricted area
}

gamma = 0.9  # Discount factor
# Initialize state values
V = {state: 0 for state in states}

"""
## Value Iteration algorithm"""

while True:
    delta = 0
    for state in states:
        if state in rewards:
            continue

        max_value = float('-inf')
        for action in actions:
            new_value = 0
            for next_action, prob in transition_probs[action].items():
                next_state = state
                if next_action == 'Up':
                    next_state = (max(state[0] - 1, 0), state[1])
                elif next_action == 'Down':
                    next_state = (min(state[0] + 1, GRID_SIZE - 1), state[1])
                elif next_action == 'Left':
                    next_state = (state[0], max(state[1] - 1, 0))
                elif next_action == 'Right':
                    next_state = (state[0], min(state[1] + 1, GRID_SIZE - 1))

                new_value += prob * (rewards.get(next_state, -0.5) + gamma * V[next_state])

            if new_value > max_value:
                max_value = new_value

        delta = max(delta, abs(max_value - V[state]))
        V[state] = max_value

    if delta < 1e-6:
        break

"""## Extracting the optimal policy"""

optimal_policy = {}
for state in states:
    if state in rewards:
        optimal_policy[state] = 'Exit'
    else:
        best_action = None
        best_value = float('-inf')
        for action in actions:
            action_value = 0
            for next_action, prob in transition_probs[action].items():
                next_state = state
                if next_action == 'Up':
                    next_state = (max(state[0] - 1, 0), state[1])
                elif next_action == 'Down':
                    next_state = (min(state[0] + 1, GRID_SIZE - 1), state[1])
                elif next_action == 'Left':
                    next_state = (state[0], max(state[1] - 1, 0))
                elif next_action == 'Right':
                    next_state = (state[0], min(state[1] + 1, GRID_SIZE - 1))

                action_value += transition_probs[action][next_action] * (rewards.get(next_state, -0.5) + gamma * V[next_state])

            if action_value > best_value:
                best_value = action_value
                best_action = action

        optimal_policy[state] = best_action

"""### Visualize the optimal policy using arrows on the grid"""

plt.figure(figsize=(5, 5))
for i in range(GRID_SIZE):
    for j in range(GRID_SIZE):
        if (i, j) in optimal_policy:
            if optimal_policy[(i, j)] == 'Up':
                plt.arrow(j, i + 0.5, 0, -0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Down':
                plt.arrow(j, i - 0.5, 0, 0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Left':
                plt.arrow(j + 0.5, i, -0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Right':
                plt.arrow(j - 0.5, i, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Exit':
                plt.text(j, i, 'Exit', ha='center', va='center')
plt.xlim(-0.5, GRID_SIZE - 0.5)
plt.ylim(GRID_SIZE - 0.5, -0.5)
plt.xticks(np.arange(0, GRID_SIZE, 1))
plt.yticks(np.arange(0, GRID_SIZE, 1))
plt.grid(True)
plt.title('Optimal Policy Visualization')
plt.show()

"""## **R3**

### A regular customer at point A and a premium customer at position B
"""

# Define the grid size
GRID_SIZE = 5

# Define the states for the grid
states = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]

# Define the actions: Up, Down, Left, Right
actions = ['Up', 'Down', 'Left', 'Right']

"""## Defining the transition probabilities and rewards for each action

"""

transition_probs = {
    'Up': {'Up': 0.8, 'Left': 0.1, 'Right': 0.1, 'Down': 0.0},
    'Down': {'Down': 0.8, 'Left': 0.1, 'Right': 0.1, 'Up': 0.0},
    'Left': {'Left': 0.8, 'Up': 0.1, 'Down': 0.1, 'Right': 0.0},
    'Right': {'Right': 0.8, 'Up': 0.1, 'Down': 0.1, 'Left': 0.0}
}

rewards = {
    (0, 1): 20,  # Regular customer at position A
    (1, 0): 30,  # Premium customer at position B
    (2, 2): -0.5,  # Live-in reward at starting position
    (4, 4): 10,  # Example exit state with a high reward
    (3, 1): -10  # Restricted area
}

gamma = 0.9  # Discount factor

"""## Value Iteration algorithm"""

# Initialize state values
V = {state: 0 for state in states}
while True:
    delta = 0
    for state in states:
        if state in rewards:
            continue

        max_value = float('-inf')
        for action in actions:
            new_value = 0
            for next_action, prob in transition_probs[action].items():
                next_state = state
                if next_action == 'Up':
                    next_state = (max(state[0] - 1, 0), state[1])
                elif next_action == 'Down':
                    next_state = (min(state[0] + 1, GRID_SIZE - 1), state[1])
                elif next_action == 'Left':
                    next_state = (state[0], max(state[1] - 1, 0))
                elif next_action == 'Right':
                    next_state = (state[0], min(state[1] + 1, GRID_SIZE - 1))

                new_value += prob * (rewards.get(next_state, -0.5) + gamma * V[next_state])

            if new_value > max_value:
                max_value = new_value

        delta = max(delta, abs(max_value - V[state]))
        V[state] = max_value

    if delta < 1e-6:
        break

"""## Extracting the optimal policy

"""

optimal_policy = {}
for state in states:
    if state in rewards:
        optimal_policy[state] = 'Exit'
    else:
        best_action = None
        best_value = float('-inf')
        for action in actions:
            action_value = 0
            for next_action, prob in transition_probs[action].items():
                next_state = state
                if next_action == 'Up':
                    next_state = (max(state[0] - 1, 0), state[1])
                elif next_action == 'Down':
                    next_state = (min(state[0] + 1, GRID_SIZE - 1), state[1])
                elif next_action == 'Left':
                    next_state = (state[0], max(state[1] - 1, 0))
                elif next_action == 'Right':
                    next_state = (state[0], min(state[1] + 1, GRID_SIZE - 1))

                action_value += transition_probs[action][next_action] * (rewards.get(next_state, -0.5) + gamma * V[next_state])

            if action_value > best_value:
                best_value = action_value
                best_action = action

        optimal_policy[state] = best_action

"""## Visualize the optimal policy using arrows on the grid

"""

plt.figure(figsize=(5, 5))
for i in range(GRID_SIZE):
    for j in range(GRID_SIZE):
        if (i, j) in optimal_policy:
            if optimal_policy[(i, j)] == 'Up':
                plt.arrow(j, i + 0.5, 0, -0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Down':
                plt.arrow(j, i - 0.5, 0, 0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Left':
                plt.arrow(j + 0.5, i, -0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Right':
                plt.arrow(j - 0.5, i, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')
            elif optimal_policy[(i, j)] == 'Exit':
                plt.text(j, i, 'Exit', ha='center', va='center')
plt.xlim(-0.5, GRID_SIZE - 0.5)
plt.ylim(GRID_SIZE - 0.5, -0.5)
plt.xticks(np.arange(0, GRID_SIZE, 1))
plt.yticks(np.arange(0, GRID_SIZE, 1))
plt.grid(True)
plt.title('Optimal Policy Visualization')
plt.show()

"""# **R4**

#  Runing 1000 episodes with random locations of customers
"""

# Define the grid size
GRID_SIZE = 5
# Define the states for the grid
states = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE) if (i, j) != (3, 1)]  # Excluding restricted area

# Flatten the list of tuples to a 1D array-like structure
flat_states = [item for sublist in states for item in sublist]

# Define the pickup points
pickup_points = ['A', 'B', 'C', 'D']

def generate_customer_request():
    # Generate a random customer request at any pickup point
    return np.random.choice(pickup_points)

def generate_second_request():
    # Generate a further request (with 60% probability)
    return np.random.choice([True, False], p=[0.6, 0.4])

def generate_premium_customer():
    # Generate a premium customer (with 30% probability)
    return np.random.choice([True, False], p=[0.3, 0.7])

def choose_random_position():
    # Choose a random position on the grid
    return np.random.choice(flat_states)

def value_iteration(rewards, gamma=0.9, theta=1e-6):
    V = {state: 0 for state in states}

    while True:
        delta = 0
        for state in states:
            if state in rewards:
                continue

            max_value = float('-inf')
            for action in actions:
                new_value = 0
                for next_action, prob in transition_probs[action].items():
                    next_state = state
                    if next_action == 'Up':
                        next_state = (max(state[0] - 1, 0), state[1])
                    elif next_action == 'Down':
                        next_state = (min(state[0] + 1, GRID_SIZE - 1), state[1])
                    elif next_action == 'Left':
                        next_state = (state[0], max(state[1] - 1, 0))
                    elif next_action == 'Right':
                        next_state = (state[0], min(state[1] + 1, GRID_SIZE - 1))

                    new_value += prob * (rewards.get(next_state, -0.5) + gamma * V[next_state])

                if new_value > max_value:
                    max_value = new_value

            delta = max(delta, abs(max_value - V[state]))
            V[state] = max_value

        if delta < theta:
            break

    return V

def visualize_policy(optimal_policy):
    plt.figure(figsize=(5, 5))
    for i in range(GRID_SIZE):
        for j in range(GRID_SIZE):
            if (i, j) in optimal_policy:
                if optimal_policy[(i, j)] == 'Up':
                    plt.arrow(j, i + 0.5, 0, -0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
                elif optimal_policy[(i, j)] == 'Down':
                    plt.arrow(j, i - 0.5, 0, 0.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
                elif optimal_policy[(i, j)] == 'Left':
                    plt.arrow(j + 0.5, i, -0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')
                elif optimal_policy[(i, j)] == 'Right':
                    plt.arrow(j - 0.5, i, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')
                elif optimal_policy[(i, j)] == 'Exit':
                    plt.text(j, i, 'Exit', ha='center', va='center')
    plt.xlim(-0.5, GRID_SIZE - 0.5)
    plt.ylim(GRID_SIZE - 0.5, -0.5)
    plt.xticks(np.arange(0, GRID_SIZE, 1))
    plt.yticks(np.arange(0, GRID_SIZE, 1))
    plt.grid(True)
    plt.title('Optimal Policy Visualization')
    plt.show()

def run_episodes(num_episodes):
    for episode in range(1, num_episodes + 1):
        print(f"Episode {episode}:")

        # Randomly choose the taxi's starting position (excluding restricted area)
        taxi_position = choose_random_position()
        print(f"Taxi starting position: {taxi_position}")

        # Generate the first customer request
        first_customer_request = generate_customer_request()
        print(f"First customer request: {first_customer_request}")

        # Generate a further request
        second_request = generate_second_request()
        if second_request:
            second_customer_request = generate_customer_request()
            print(f"Second customer request: {second_customer_request}")
            premium_customer = generate_premium_customer()
            if premium_customer:
                print("Second customer is a premium customer.")

        # Run value iteration for optimal policy calculation
        rewards = {
            (i, j): 20 if pickup_points.index(first_customer_request) == idx else (30 if pickup_points.index(second_customer_request) == idx else -0.5)
            for idx, (i, j) in enumerate(states)
        }
        optimal_policy = value_iteration(rewards)

        # Visualize the optimal policy for each episode
        visualize_policy(optimal_policy)
        print("\n")

# Run 1000 episodes
run_episodes(1000)

